"""
Polish Text Preprocessor for Audiobook Production
Standalone tool to clean and prepare Polish text for TTS processing.
"""

import os
import sys
from typing import List, Optional
from pathlib import Path
import openai
from config import Config

class PolishTextPreprocessor:
    """Preprocesses Polish text for audiobook production using OpenAI."""
    
    def __init__(self):
        """Initialize the preprocessor with OpenAI configuration."""
        if not Config.validate():
            raise ValueError("OpenAI API key not configured. Please set OPENAI_API_KEY in your environment.")
        
        self.client = openai.OpenAI(api_key=Config.OPENAI_API_KEY)
        self.model = Config.OPENAI_MODEL
        
    def get_cleanup_prompt(self) -> str:
        """Get the system prompt for text cleanup."""
        return """JesteÅ› ekspertem od przygotowywania polskich tekstÃ³w na audiobooki. 
    Twoim zadaniem jest oczyszczenie tekstu akademickiego/historycznego tak, aby byÅ‚ idealny do syntezy mowy.

    ZASADY CZYSZCZENIA:

    1. **SkrÃ³ty do rozwiniÄ™cia**:
       - "itd." â†’ "i tak dalej"
       - "itp" â†’ "i tym podobne"
       - "np." â†’ "na przykÅ‚ad"
       - "tzn." â†’ "to znaczy"
       - "m.in." â†’ "miÄ™dzy innymi"
       - "tj." â†’ "to jest"
       - "tzw." â†’ "tak zwany"
       - "przyp." â†’ "przypis"
       - "wyd." â†’ "wydanie"
       - "cyt." â†’ "cytowane"
       - "s." â†’ "strona" (gdy oznacza numer strony)
       - "dr" / "dr." â†’ "doktor"
       - "prof." â†’ "profesor"
       - "mgr" / "mgr." â†’ "magister"
       - "inÅ¼." â†’ "inÅ¼ynier"
       - "mgr inÅ¼." â†’ "magister inÅ¼ynier"
       - "pt." â†’ "pod tytuÅ‚em"
       - "por." â†’ "porÃ³wnaj" (w kontekÅ›cie akademickim)
       - "nt." â†’ "na temat"

    2. **LICZBY DO TEKSTU** (BARDZO WAÅ»NE dla TTS):
       - Lata: "1946" â†’ dostosuj do kontekstu gramatycznego:
         * "w 1946 roku" â†’ "w tysiÄ…c dziewiÄ™Ä‡set czterdziestym szÃ³stym roku"
         * "rok 1946" â†’ "rok tysiÄ…c dziewiÄ™Ä‡set czterdziesty szÃ³sty"
         * "od 1946" â†’ "od tysiÄ…c dziewiÄ™Ä‡set czterdziestego szÃ³stego"
       - Daty: "11 wrzeÅ›nia 2001 r." â†’ "jedenastego wrzeÅ›nia dwa tysiÄ…ce pierwszego roku"
       - Liczby podstawowe: "15" â†’ "piÄ™tnaÅ›cie", "301" â†’ "trzysta jeden"
       - Procenty: "25%" â†’ "dwadzieÅ›cia piÄ™Ä‡ procent"
       - DziesiÄ™tne: "3.5" â†’ "trzy i piÄ™Ä‡ dziesiÄ…tych"
       - Numeracja: "1." â†’ "pierwszy", "2." â†’ "drugi" (gdy to punkty listy)
       - Setki/tysiÄ…ce: "2000" â†’ "dwa tysiÄ…ce"
       - Cyfry rzymskie: "XX wieku" â†’ "dwudziestego wieku", "II RP" â†’ "druga eR Pe", 
         "III Rzesza" â†’ "trzecia Rzesza", "IV rozbiÃ³r" â†’ "czwarty rozbiÃ³r"
       - Liczby porzÄ…dkowe z myÅ›lnikiem: "10-ty" â†’ "dziesiÄ…ty", "11-ty" â†’ "jedenasty", "12-ty" â†’ "dwunasty"
       - Listy numerowane: "Ad 1)" â†’ "punkt pierwszy", "Ad 2)" â†’ "punkt drugi"
       - RÃ³wnania: "(1)..." â†’ "rÃ³wnanie pierwsze", "(2)..." â†’ "rÃ³wnanie drugie"

    3. **AKRONIMY DO FONETYKI** (WAÅ»NE dla TTS):
       Wszystkie akronimy przeliteruj fonetycznie po polsku:
       - "USA" â†’ "U eS A"
       - "ZSRR" â†’ "Zet eS eR eR"
       - "PRL" â†’ "Pe eR eL"
       - "UE" â†’ "U E"
       - "ONZ" â†’ "O eN Zet"
       - "WTC" â†’ "Wu Te Ce"
       - "IPS" â†’ "I Pe eS"
       - "MWG" â†’ "eM Wu Gie" 
       - "MWD" â†’ "eM Wu De"
       - "KI" â†’ "Ka I"
       - "GRU" â†’ "Gie eR U"
       - "UN" â†’ "U eN"
       - "EU" â†’ "E U"
       - "NKWD" â†’ "eN Ka Wu De"
       - "MGB" â†’ "eM Gie Be"
       - "ChRL" â†’ "Ce Ha eR eL"
       
       WyjÄ…tki (pozostaw bez zmian):
       - "NATO" â†’ "NATO" (powszechnie znany skrÃ³t)

    4. **FORMATOWANIE TEKSTU**:
       - UsuÅ„ niepotrzebne spacje miÄ™dzy literami w sÅ‚owach:
         * "Å› w i a t o p o g l Ä… d u" â†’ "Å›wiatopoglÄ…du"
         * "o g Ã³ l n y m o d e l" â†’ "ogÃ³lny model"
         * "p o z y c j a c z Å‚ o w i e k a" â†’ "pozycja czÅ‚owieka"
       - Zachowaj normalne spacje miÄ™dzy sÅ‚owami.
       - Popraw bÅ‚Ä™dy formatowania, takie jak "nie zamiesCzym jest nauka porÃ³wnawczazczaÅ‚em" â†’ "nie zamieszczaÅ‚em".
       - InicjaÅ‚y w imionach: "G. W. Bush" â†’ "Gie Wu Bush", "F. Konecznego" â†’ "Ef Konecznego".

    5. **Usuwanie**:
       - Wszystkie numery przypisÃ³w: Â¹, Â², Â³, 1, 2, 3 (gdy sÄ… przypisami)
       - Bibliografia i cytowania w nawiasach, np. "Por. J. Kossecki, Cybernetyka spoÅ‚eczna, Warszawa 1981, s. 79-80, 82-96."
       - NumeracjÄ™ rozdziaÅ‚Ã³w/sekcji na poczÄ…tku linii.
       - Referencje do stron typu "(s. 301)".
       - Samodzielne numery stron wstawione w tekst (np. "92" w osobnej linii).
       - Cytowania akademickie: "Por. tamÅ¼e, s. 305.", "wyd. cyt.", "Por. OBWIESZCZENIE..."
       - Puste nawiasy przerwaÅ„ "(...)" w tekÅ›cie.
       - Formalne tytuÅ‚y dokumentÃ³w pisane WIELKIMI LITERAMI gdy przeszkadzajÄ… w narracji.

    6. **Zachowywanie**:
       - Naturalny przepÅ‚yw narracji.
       - Kontekst historyczny i merytoryczny.
       - Nazwy wÅ‚asne osÃ³b i miejsc.
       - Znaczenie i sens treÅ›ci.
       - ObcojÄ™zyczne zwroty i tytuÅ‚y w oryginale, np. "The Pacularity of Men".

    7. **Ulepszanie dla audio**:
       - PÅ‚ynne przejÅ›cia miÄ™dzy zdaniami.
       - Usuwanie zagnieÅ¼dÅ¼onych nawiasÃ³w zakÅ‚Ã³cajÄ…cych narracjÄ™.
       - PrzeksztaÅ‚canie przypisÃ³w na naturalny tekst (tylko jeÅ›li waÅ¼ne merytorycznie).
       - Wszystkie liczby wymÃ³wione sÅ‚owami w odpowiednim przypadku gramatycznym.
       - Akronimy przeliterowane fonetycznie dla lepszej wymowy.

    ODPOWIEDÅ¹: ZwrÃ³Ä‡ TYLKO oczyszczony tekst, bez Å¼adnych komentarzy czy objaÅ›nieÅ„."""

    def chunk_text(self, text: str, max_chunk_size: int = 2500) -> List[str]:
        """
        Split text into chunks for processing.
        
        Optimized chunk size (2.5K chars) for best LLM processing reliability.
        Even though GPT-4o-mini supports 128K tokens, smaller chunks provide:
        - Better attention and focus
        - More consistent processing quality  
        - Improved reliability and error handling
        - Prevents content truncation issues
        
        Note: Chapter splitting is handled manually - this processes the full text.
        """
        if len(text) <= max_chunk_size:
            return [text]
        
        # Process full text using paragraph-based splitting
        return self._split_large_text(text, max_chunk_size)
    
    def _split_large_text(self, text: str, max_chunk_size: int) -> List[str]:
        """Split large text by paragraphs, then sentences if needed."""
        chunks = []
        
        # Try to split by paragraphs first
        paragraphs = text.split('\n\n')
        current_chunk = ""
        
        for paragraph in paragraphs:
            # Check if adding this paragraph would exceed size
            if len(current_chunk) + len(paragraph) + 2 <= max_chunk_size:
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph
            else:
                # Save current chunk if it exists
                if current_chunk:
                    chunks.append(current_chunk)
                
                # Handle oversized paragraph
                if len(paragraph) > max_chunk_size:
                    # Split by sentences
                    sentences = paragraph.split('.')
                    current_chunk = ""
                    
                    for sentence in sentences:
                        if len(current_chunk) + len(sentence) + 1 <= max_chunk_size:
                            if current_chunk:
                                current_chunk += "." + sentence
                            else:
                                current_chunk = sentence
                        else:
                            if current_chunk:
                                chunks.append(current_chunk)
                            current_chunk = sentence
                else:
                    current_chunk = paragraph
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks

    def clean_text_chunk(self, text_chunk: str, debug: bool = False) -> str:
        """Clean a single chunk of text using OpenAI."""
        try:
            if debug:
                print(f"    ğŸ” DEBUG: Input chunk length: {len(text_chunk)} chars")
                print(f"    ğŸ” DEBUG: Input preview: {text_chunk[:100]}...")
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": self.get_cleanup_prompt()},
                    {"role": "user", "content": f"OczyÅ›Ä‡ ten tekst:\n\n{text_chunk}"}
                ],
                temperature=0.3,  # Lower temperature for more consistent cleaning
                max_tokens=8000  # Increased token limit to handle larger chunks
            )
            
            cleaned_text = response.choices[0].message.content.strip()
            
            if debug:
                print(f"    ğŸ” DEBUG: Output length: {len(cleaned_text)} chars")
                print(f"    ğŸ” DEBUG: Output preview: {cleaned_text[:100]}...")
            
            # Check if we got a reasonable response
            if len(cleaned_text) < 10:  # Very short response is suspicious
                print(f"    âš ï¸  WARNING: Very short response ({len(cleaned_text)} chars), using original")
                return text_chunk
            
            return cleaned_text
            
        except Exception as e:
            print(f"    âŒ Error cleaning text chunk: {e}")
            return text_chunk  # Return original if cleaning fails

    def process_file(self, input_file: str, chunk_size: int = 2500, debug: bool = False) -> bool:
        """Process a single text file."""
        input_path = Path(input_file)
        
        if not input_path.exists():
            print(f"âŒ File not found: {input_file}")
            return False
        
        # Create output filename with _clean suffix
        output_path = input_path.parent / f"{input_path.stem}_clean{input_path.suffix}"
        
        print(f"ğŸ“– Reading: {input_path}")
        
        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                original_text = f.read()
        except Exception as e:
            print(f"âŒ Error reading file: {e}")
            return False
        
        print(f"ğŸ“ Original text length: {len(original_text)} characters")
        
        # Split into chunks for processing
        chunks = self.chunk_text(original_text, chunk_size)
        print(f"ğŸ”„ Processing {len(chunks)} chunks...")
        
        cleaned_chunks = []
        for i, chunk in enumerate(chunks, 1):
            print(f"   Processing chunk {i}/{len(chunks)}...")
            if debug:
                print(f"    ğŸ” DEBUG: Processing chunk {i}")
            cleaned_chunk = self.clean_text_chunk(chunk, debug=debug)
            cleaned_chunks.append(cleaned_chunk)
            if debug:
                print(f"    ğŸ” DEBUG: Chunk {i} added to results (length: {len(cleaned_chunk)})")
                print(f"    ğŸ” DEBUG: Total chunks so far: {len(cleaned_chunks)}")
                print("    " + "="*50)
        
        # Combine cleaned chunks
        cleaned_text = "\n\n".join(cleaned_chunks)
        
        print(f"âœ¨ Cleaned text length: {len(cleaned_text)} characters")
        
        # Save cleaned text
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(cleaned_text)
            print(f"âœ… Saved cleaned text: {output_path}")
            return True
        except Exception as e:
            print(f"âŒ Error saving file: {e}")
            return False

def estimate_cost(text_length: int, model: str = "gpt-4o-mini") -> float:
    """Estimate processing cost for the given text."""
    # Rough token estimation: 1 token â‰ˆ 4 characters for Polish
    tokens = text_length / 4
    
    # Pricing per million tokens (input + output estimated)
    pricing = {
        "gpt-4.1-nano": {"input": 0.10, "output": 0.40},
        "gpt-4o-mini": {"input": 0.15, "output": 0.60},
        "gpt-4.1-mini": {"input": 0.40, "output": 1.60}
    }
    
    rates = pricing.get(model, pricing["gpt-4o-mini"])
    
    # Estimate output as 80% of input (cleaning usually reduces text)
    input_cost = (tokens / 1_000_000) * rates["input"]
    output_cost = (tokens * 0.8 / 1_000_000) * rates["output"]
    
    return input_cost + output_cost

def main():
    """Main function for standalone usage."""
    import argparse
    
    parser = argparse.ArgumentParser(description="ğŸ¯ Polish Text Preprocessor for Audiobooks")
    parser.add_argument("input_file", help="Input text file to process")
    parser.add_argument("--chunk-size", type=int, default=2500, 
                       help="Maximum chunk size in characters (default: 2500, optimized for reliability)")
    parser.add_argument("--estimate-cost", action="store_true",
                       help="Show cost estimate before processing")
    parser.add_argument("--model", default="gpt-4o-mini",
                       choices=["gpt-4.1-nano", "gpt-4o-mini", "gpt-4.1-mini"],
                       help="OpenAI model to use (default: gpt-4o-mini)")
    parser.add_argument("--save-chunks-only", action="store_true",
                       help="Save text chunks to file for inspection without processing them")
    parser.add_argument("--debug", action="store_true",
                       help="Enable debug output to see what OpenAI returns for each chunk")
    
    # If no arguments provided, show help
    if len(sys.argv) == 1:
        parser.print_help()
        return
    
    args = parser.parse_args()
    
    # If no path specified, assume text_sources directory
    input_file = args.input_file
    if not os.path.dirname(input_file):
        input_file = os.path.join("text_sources", input_file)
    
    # Check if file exists
    if not os.path.exists(input_file):
        print(f"âŒ File not found: {input_file}")
        return
    
    # Read file to get length for cost estimation
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            text_content = f.read()
    except Exception as e:
        print(f"âŒ Error reading file: {e}")
        return
    
    print(f"ğŸ“– File: {input_file}")
    print(f"ğŸ“ Text length: {len(text_content):,} characters")
    
    # Cost estimation
    estimated_cost = estimate_cost(len(text_content), args.model)
    print(f"ğŸ’° Estimated cost ({args.model}): ${estimated_cost:.4f}")
    
    if args.estimate_cost:
        print("\nğŸ’¡ Cost comparison:")
        for model in ["gpt-4.1-nano", "gpt-4o-mini", "gpt-4.1-mini"]:
            cost = estimate_cost(len(text_content), model)
            print(f"   {model}: ${cost:.4f}")
        
        confirm = input("\nğŸ”„ Proceed with processing? (y/N): ")
        if confirm.lower() != 'y':
            print("âŒ Processing cancelled.")
            return
    
    # Handle save-chunks-only mode
    if args.save_chunks_only:
        print("ğŸ” CHUNKS ONLY MODE: Saving chunks for inspection (no processing)")
        
        # Create a temporary preprocessor instance just for chunking
        preprocessor = PolishTextPreprocessor()
        
        # Use custom chunk size if specified
        if args.chunk_size != 2500:
            print(f"ğŸ”§ Using custom chunk size: {args.chunk_size:,} characters")
        
        # Split into chunks
        chunks = preprocessor.chunk_text(text_content, args.chunk_size)
        print(f"ğŸ“ Split text into {len(chunks)} chunks")
        
        # Save chunks to file for inspection
        chunks_file = input_file.replace('.txt', '_chunks_debug.txt')
        try:
            with open(chunks_file, 'w', encoding='utf-8') as f:
                f.write(f"=== DEBUG CHUNKS FOR INSPECTION ===\n")
                f.write(f"Source file: {input_file}\n")
                f.write(f"Total chunks: {len(chunks)}\n")
                f.write(f"Chunk size setting: {args.chunk_size}\n")
                f.write(f"Original text length: {len(text_content)} characters\n\n")
                
                for i, chunk in enumerate(chunks):
                    chunk_len = len(chunk)
                    f.write(f"=== CHUNK {i+1}/{len(chunks)} ({chunk_len} characters) ===\n")
                    f.write(f"First 100 chars: {chunk[:100]}...\n")
                    f.write(f"Last 100 chars: ...{chunk[-100:]}\n")
                    f.write(f"--- FULL CHUNK CONTENT ---\n")
                    f.write(f"{chunk}\n\n")
                
                f.write(f"=== END OF CHUNKS ===\n")
            
            print(f"âœ… Chunks saved to: {chunks_file}")
            print(f"ğŸ“Š Summary:")
            for i, chunk in enumerate(chunks):
                chunk_len = len(chunk)
                preview = chunk[:50] + "..." if len(chunk) > 50 else chunk
                print(f"  Chunk {i+1}: {preview} ({chunk_len} chars)")
            return
            
        except Exception as e:
            print(f"âŒ Error saving chunks file: {e}")
            return
    
    # Update config if different model specified
    if args.model != Config.OPENAI_MODEL:
        print(f"ğŸ”§ Using model: {args.model}")
        Config.OPENAI_MODEL = args.model
    
    # Process the file
    preprocessor = PolishTextPreprocessor()
    
    # Use custom chunk size if specified
    if args.chunk_size != 2500:
        print(f"ğŸ”§ Using custom chunk size: {args.chunk_size:,} characters")
    
    success = preprocessor.process_file(input_file, args.chunk_size, debug=args.debug)
    
    if success:
        print()
        print("ğŸ‰ Text preprocessing completed!")
        print("ğŸ“ You can now run TTS on the cleaned file.")
        print(f"ğŸ’° Actual cost: ~${estimated_cost:.4f}")
    else:
        print()
        print("âŒ Text preprocessing failed!")
        sys.exit(1)

if __name__ == "__main__":
    main()